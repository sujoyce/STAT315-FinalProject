{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfAgliHOZXbL"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1tNip83cEn7",
        "outputId": "150123d1-9fff-4d1f-e84f-4784638977bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/STAT315_FinalProject\n"
          ]
        }
      ],
      "source": [
        "austin = pd.read_csv(\"listings.csv\", index_col=0)\n",
        "df = pd.DataFrame(austin)\n",
        "random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Project Introduction and Data Source**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our study is motivated by the need to analyze Airbnb pricing in Austin, TX and gain a better understanding of what factors impact these rates. To do this, we used Airbnb.com data collected from October to November 2021. This data was scraped from the website and recorded. In total, the dataset had 16 columns with data. This included factors such as price, neighborhood, room type, minimum nights of stay available, total number of reviews, number of host listings, yearly availability, and number of reviews left by users in the last 12 months.\n",
        "\n",
        "To protect location anonymity, Airbnb anonymizes location information for listings. Because of this, the location for a listing in the data will be 0-450 feet from the actual address. Furthermore, listings in the same building are anonymized individually, so they may appear scattered around the actual address.\n",
        "\n",
        "For our analysis, we decided to explore how different variables, such as location factors, affect our target variable price. We hope that an understanding of these variables and their trends can help users book more affordable rentals and facilitate reasonable rental pricing by hosts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop any rows with missing latitude, longitude, or price\n",
        "df = df.dropna(subset=['latitude', 'longitude', 'price'])\n",
        "\n",
        "df = df.drop(columns=['license', 'host_name', 'name', 'host_id', 'id'], errors='ignore')\n",
        "\n",
        "df['price'] = df['price'].fillna(df['price'].median())\n",
        "df['reviews_per_month'] = df['reviews_per_month'].fillna(df['reviews_per_month'].median())\n",
        "\n",
        "df['log_minimum_nights'] = np.log1p(df['minimum_nights'])\n",
        "df['log_number_of_reviews'] = np.log1p(df['number_of_reviews'])\n",
        "df['log_price'] = np.log1p(df['price'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Cleaning and Preparation**\n",
        "\n",
        "After exploring the data, we realized that we had missing values in the neigh-\n",
        "borhood groups, reviews per month, and price columns.\n",
        "Since the neighborhood groups column actually had no data points, we ended\n",
        "up dropping the entire column. We filled in the missing values with 0 for\n",
        "the reviews per month column, assuming that the missing data points had no\n",
        "reviews. For the price column, we imputed the missing data using the median\n",
        "price value.\n",
        "a\n",
        "We discovered that there was high right skew in the minimum nights, number of\n",
        "reviews, and price columns. As a result, we decided to do a log transformation\n",
        "on all three. Since there were outliers on price, our response variable, we removed listings\n",
        "that were over $1000 per night.\n",
        "\n",
        "After seeing the importance of lattitude and longitude to our MLP we exper-\n",
        "imented with creating new features from them that would help with interpre-\n",
        "tation and hopefully would increase the performance of our model. Our first\n",
        "attempt was to simply find the coordinates for downtown Austin and then mea-\n",
        "sure listing’s distance from there and record them in the column \"distance to\n",
        "center\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Exploratory Data Analysis***\n",
        "\n",
        "Before we delve into complex modeling, we felt that it would be beneficial to gain a deeper understanding of our target variable. To do this, we begain with some exploratory data analysis focusing on the distriibution of price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df['price'].dropna(), bins=50, kde=True) # Plots the distribution of price\n",
        "\n",
        "plt.title('Distribution of Listing Prices', fontsize=18)\n",
        "plt.xlabel('Price ($)', fontsize=14)\n",
        "plt.ylabel('Number of Listings', fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this original plot, we observed that the distribution of price is very left skewed. This could indicate the presence of high outliers. To better account for this, we tried a similiar plot with the natural log of price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df['price'].dropna(), bins=50, kde=True, log_scale=(True, False)) #Puts the x-axis in log scale so the distribution isn't so clustered\n",
        "\n",
        "plt.title('Distribution of Listing Prices', fontsize=18)\n",
        "plt.xlabel('Price ($)', fontsize=14)\n",
        "plt.ylabel('Number of Listings', fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking the natural log of price provided a drastically improved visual to see the shape of listing prices. This transformed distribution revealed a more balanced spread of listing prices, highlighting a clear central tendency and reducing the visual dominance of high-end outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "df_filtered = df[df['price'] > 0] # Only keeps listings with positive prices\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.histplot(\n",
        "    data=df_filtered,\n",
        "    x='price',\n",
        "    hue='room_type', # color by room type\n",
        "    bins=50,\n",
        "    kde=True,\n",
        "    log_scale=(True, False)\n",
        ")\n",
        "\n",
        "plt.title('Distribution of Listing Prices by Room Type (Log Scale)', fontsize=18)\n",
        "plt.xlabel('Price ($) - Log Scale', fontsize=14)\n",
        "plt.ylabel('Number of Listings', fontsize=14)\n",
        "\n",
        "plt.legend(title='Room Type', labels=df_filtered['room_type'].unique())\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "top_zipcodes = df_filtered['neighbourhood'].value_counts().head(5).index.tolist()\n",
        "\n",
        "df_zip = df_filtered[df_filtered['neighbourhood'].isin(top_zipcodes)].copy()\n",
        "df_zip['neighbourhood'] = df_zip['neighbourhood'].astype(str)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.boxplot(\n",
        "    data=df_zip,\n",
        "    x='neighbourhood',\n",
        "    y='price',        \n",
        "    palette='Set2'   \n",
        ")\n",
        "\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.title('Listing Prices by ZIP Code (Top 5)', fontsize=18)\n",
        "plt.xlabel('ZIP Code', fontsize=14)\n",
        "plt.ylabel('Price ($) - Log Scale', fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we wanted to visualize how price is distributed across the top five most expensive ZIP codes in the Austin, TX metropolitan area. To assist with visibility and account for outliers, we decided to again take the natural log of price.\n",
        "\n",
        "This boxplot shows a lot of price variation within each of these ZIP codes. Comparitively, across these codes, ZIP 78701 has the highest median price. This ZIP code specifically also has the largest spread of prices, while ZIP 78745 has the smallest spread. Each ZIP code group still has a large spread, likely due to the large amount of outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "top_zipcodes = df_filtered['neighbourhood'].value_counts().head(5).index.tolist()\n",
        "\n",
        "df_zip = df_filtered[df_filtered['neighbourhood'].isin(top_zipcodes)].copy()\n",
        "\n",
        "df_zip['neighbourhood'] = df_zip['neighbourhood'].astype(str)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.histplot(\n",
        "    data=df_zip,\n",
        "    x='price',\n",
        "    hue='neighbourhood',\n",
        "    bins=50,\n",
        "    kde=True,\n",
        "    log_scale=(True, False)\n",
        ")\n",
        "\n",
        "plt.title('Distribution of Listing Prices by ZIP Code (Top 5)', fontsize=18)\n",
        "plt.xlabel('Price ($) - Log Scale', fontsize=14)\n",
        "plt.ylabel('Number of Listings', fontsize=14)\n",
        "\n",
        "plt.legend(title='ZIP Code')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to our previous plot, we utilized a box plot to visualize the distribution of pricing, this time across room types. From this, we can see that entire rooms/apartments are expensive on average but have a high variation between prices. Private rooms are the second cheapest on average, but also have a high price variation. However, shared rooms are on average the cheapest and are relatively consistent in price, especially compared to other room types. In contrast, hotel rooms are on average the most expensive, but have the most consistent price rates. Most importantly, it is important to observe how the majority of our price variation occurs in entire homes/apartments and private rooms. These groups simultaneously contain some of our most expensive and cheapest options, indicating that room type alone is probably not the only factor contributing to price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Make a HeatMap data list\n",
        "heat_data = [[row['latitude'], row['longitude'], row['price']] for index, row in df.iterrows()]\n",
        "\n",
        "# Step 5: Create a base map centered around Austin\n",
        "m = folium.Map(location=[30.2672, -97.7431], zoom_start=12)\n",
        "\n",
        "# Step 6: Add the HeatMap\n",
        "HeatMap(heat_data,\n",
        "        radius=15,\n",
        "        blur=10,\n",
        "        max_zoom=1).add_to(m)\n",
        "# Step 7: Add Tourist Attractions\n",
        "tourist_attractions = [\n",
        "    {\"name\": \"Texas State Capitol\", \"lat\": 30.2747, \"lon\": -97.7404},\n",
        "    {\"name\": \"Lady Bird Lake\", \"lat\": 30.2604, \"lon\": -97.7490},\n",
        "    {\"name\": \"Zilker Park\", \"lat\": 30.2669, \"lon\": -97.7725},\n",
        "    {\"name\": \"Barton Springs Pool\", \"lat\": 30.2644, \"lon\": -97.7720},\n",
        "    {\"name\": \"South Congress Avenue\", \"lat\": 30.2491, \"lon\": -97.7496},\n",
        "    {\"name\": \"The University of Texas at Austin\", \"lat\": 30.2850, \"lon\": -97.7345},\n",
        "    {\"name\": \"Sixth Street Historic District\", \"lat\": 30.2676, \"lon\": -97.7393},\n",
        "    {\"name\": \"Mount Bonnell\", \"lat\": 30.3215, \"lon\": -97.7737},\n",
        "    {\"name\": \"Blanton Museum of Art\", \"lat\": 30.2803, \"lon\": -97.7370}\n",
        "]\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'lightred', 'cadetblue', 'pink']\n",
        "\n",
        "# Add markers for each tourist attraction with a different color\n",
        "for idx, place in enumerate(tourist_attractions):\n",
        "    folium.Marker(\n",
        "        location=[place[\"lat\"], place[\"lon\"]],\n",
        "        popup=place[\"name\"],\n",
        "        icon=folium.Icon(color=colors[idx % len(colors)], icon='info-sign')\n",
        "    ).add_to(m)\n",
        "\n",
        "# Show the map\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our last exploratory analysis, we wanted to explore the pricing distribution on a geo-spatial level. First, we noticed that most of our more expensive lisitngs were centralized around the \"downtown\" Austin area, with cheaper rentals dispersed across less \"touristy\" areas. From this heat map, we also observed some gapping throughout unexpected areas. This left us wondering how proximity to recreational attractions may affect price and rental locations. So, we were able to obtain coordinates of popular attractions in Austin and overlay them on our map. This did potentially explain some of our observed trends, including the gapping, as some attractions are on private land that typically do not host Airbnbs. For example, the University of Texas and the Texas State Capitol are not available for Airbnbs due to the presence of student housing and ownership by the state. It makes sense that many renters are willing to pay more to be as close as possible to the city's attractions, so Airbnb hosts listing as close as possible to them are seemingly pricing these properties higher on average than those that are further away (for example, Mount Bonnell)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dfmlp = pd.DataFrame(austin)\n",
        "\n",
        "#drop useless columns\n",
        "dfmlp = dfmlp.drop(columns=['license', 'host_name', 'name', 'host_id', 'id'], errors='ignore')\n",
        "\n",
        "#created clusters of close together landmarks \n",
        "clusters = {\n",
        "    \"downtown_cluster\": [\n",
        "        (30.2747, -97.7404),  # Texas State Capitol\n",
        "        (30.2604, -97.7490),  # Lady Bird Lake\n",
        "        (30.2676, -97.7393),  # Sixth Street Historic District\n",
        "        (30.2850, -97.7345),  # University of Texas at Austin\n",
        "        (30.2803, -97.7370)   # Blanton Museum of Art\n",
        "    ],\n",
        "    \"zilker_cluster\": [\n",
        "        (30.2669, -97.7725),  # Zilker Park\n",
        "        (30.2644, -97.7720)   # Barton Springs Pool\n",
        "    ],\n",
        "    \"northwest_cluster\": [\n",
        "        (30.3215, -97.7737)   # Mount Bonnell\n",
        "    ],\n",
        "    \"south_congress_cluster\": [\n",
        "        (30.2491, -97.7496)   # South Congress Avenue\n",
        "    ]\n",
        "}\n",
        "\n",
        "# transformed last review day to days since last review\n",
        "dfmlp['last_review'] = pd.to_datetime(dfmlp['last_review'], errors='coerce')\n",
        "today = pd.to_datetime('today')\n",
        "dfmlp['days_since_last_review'] = (today - dfmlp['last_review']).dt.days\n",
        "dfmlp['days_since_last_review'] = dfmlp['days_since_last_review'].fillna(dfmlp['days_since_last_review'].max())\n",
        "\n",
        "\n",
        "dfmlp = dfmlp.drop(columns=['last_review'])\n",
        "\n",
        "# filled reviews_per_month na values with 0\n",
        "dfmlp['reviews_per_month'] = dfmlp['reviews_per_month'].fillna(0)\n",
        "\n",
        "# function to calculate distance between lattitude\n",
        "def calculate_distance(lat1, lon1, lat2, lon2):\n",
        "    return np.sqrt((69 * (lat1 - lat2))**2 + (54.6 * (lon1 - lon2))**2)\n",
        "\n",
        "# Create distance to each cluster\n",
        "for cluster_name, locations in clusters.items():\n",
        "    distances = [calculate_distance(dfmlp['latitude'], dfmlp['longitude'], lat, lon) for lat, lon in locations]\n",
        "    # Take the minimum distance to any point in the cluster\n",
        "    dfmlp[f\"dist_to_{cluster_name}\"] = np.min(distances, axis=0)\n",
        "\n",
        "# transform skewed factors\n",
        "dfmlp['log_minimum_nights'] = np.log1p(df['minimum_nights'])\n",
        "dfmlp['log_number_of_reviews'] = np.log1p(df['number_of_reviews'])\n",
        "\n",
        "# list factors by type\n",
        "categorical = ['neighbourhood', 'neighbourhood_group', 'room_type']\n",
        "numerical = [\n",
        "    'log_minimum_nights', 'log_number_of_reviews', 'calculated_host_listings_count',\n",
        "    'availability_365', 'number_of_reviews_ltm', 'reviews_per_month',\n",
        "    'latitude', 'longitude', 'days_since_last_review',\n",
        "    'dist_to_downtown_cluster', 'dist_to_zilker_cluster', 'dist_to_northwest_cluster', 'dist_to_south_congress_cluster'\n",
        "]\n",
        "\n",
        "# removed outliers\n",
        "price_cap = 1000  \n",
        "dfmlp = dfmlp[dfmlp['price'] <= price_cap]\n",
        "\n",
        "# drop rows with missing price data \n",
        "df_cleanmlp = dfmlp[categorical + numerical + ['price']].dropna(subset=['price'])\n",
        "\n",
        "# log transform price because it is still skewed \n",
        "X = df_cleanmlp[categorical + numerical]\n",
        "y = np.log1p(df_cleanmlp['price'])  \n",
        "\n",
        "#hot one encoding my categorical variables\n",
        "temp_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "temp_encoder.fit(X[categorical])\n",
        "\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  \n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical),  # numeric features: impute + scale\n",
        "        ('cat', OneHotEncoder(categories=temp_encoder.categories_, handle_unknown='ignore', sparse_output=False), categorical)  # categorical: one-hot encode\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipeline_for_tuning = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', MLPRegressor(max_iter=10000, random_state=42))\n",
        "])\n",
        "\n",
        "# grid of parameters for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'regressor__hidden_layer_sizes': [(50,), (100,), (100, 50), (150, 100)],\n",
        "    'regressor__alpha': [0.0001, 0.001, 0.01],\n",
        "    'regressor__learning_rate_init': [0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "#grid search to run all combinations of hyperparameters\n",
        "search = GridSearchCV(\n",
        "    pipeline_for_tuning,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='r2',\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "search.fit(X, y)\n",
        "\n",
        "print(\"\\nBest parameters from tuning:\")\n",
        "print(search.best_params_)\n",
        "\n",
        "# regression model\n",
        "best_mlp = MLPRegressor(\n",
        "    hidden_layer_sizes=search.best_params_['regressor__hidden_layer_sizes'],\n",
        "    alpha=search.best_params_['regressor__alpha'],\n",
        "    learning_rate_init=search.best_params_['regressor__learning_rate_init'],\n",
        "    max_iter=10000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', best_mlp)\n",
        "])\n",
        "\n",
        "# kfolds resampling method\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# holders for performance metrics\n",
        "importances_list = []\n",
        "mae_list = []\n",
        "rmse_list = []\n",
        "r2_list = []\n",
        "\n",
        "feature_names = None \n",
        "\n",
        "# run kfolds \n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    \n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "   \n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_pred = np.expm1(y_pred)\n",
        "    y_test = np.expm1(y_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    mae_list.append(mae)\n",
        "    rmse_list.append(rmse)\n",
        "    r2_list.append(r2)\n",
        "\n",
        "    \n",
        "    X_test_transformed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
        "\n",
        "    \n",
        "    result = permutation_importance(\n",
        "        pipeline.named_steps['regressor'],\n",
        "        X_test_transformed,\n",
        "        y_test,\n",
        "        n_repeats=10,\n",
        "        random_state=22,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    importances_list.append(result.importances_mean)\n",
        "\n",
        "    \n",
        "    if feature_names is None:\n",
        "        cat_features = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical)\n",
        "        num_features = numerical\n",
        "        feature_names = list(num_features) + list(cat_features)\n",
        "\n",
        "    print(f\"Fold {fold} completed. MAE={mae:.2f}, RMSE={rmse:.2f}, R²={r2:.3f}\")\n",
        "\n",
        "\n",
        "mean_importances = np.mean(importances_list, axis=0)\n",
        "\n",
        "assert len(feature_names) == len(mean_importances), f\"Mismatch: {len(feature_names)} names vs {len(mean_importances)} importances.\"\n",
        "\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': mean_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "\n",
        "print(\"\\nAverage Model Performance Across Folds:\")\n",
        "print(f\"Average MAE: {np.mean(mae_list):.2f} ± {np.std(mae_list):.2f}\")\n",
        "print(f\"Average RMSE: {np.mean(rmse_list):.2f} ± {np.std(rmse_list):.2f}\")\n",
        "print(f\"Average R²: {np.mean(r2_list):.3f} ± {np.std(r2_list):.3f}\")\n",
        "\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Fold': list(range(1, 6)),\n",
        "    'MAE': mae_list,\n",
        "    'RMSE': rmse_list,\n",
        "    'R2': r2_list\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to get a better understanding about how our factors impacted price we trained two models, linear regression and multilayer perceptron (MLP). We used a k-folds cross validation approach to training both models in order to get a stable estimate of their respecitve performances. In order to have a fair comparison we also made sure to train both models on identical feature sets. We used GridSearchCV in order to test a wide array of hyperparameters for the MLP models as well as preparing the factors, but both the MLP and the linear regression failed to get RMSE's of less than 600. Running a variable importance plot we found that latitude and longitude were the most important features especially for the MLP. We attempted to make this location information more interpretable and increase model performance by finding the coordinates for downtown Austin as defined by Apple Maps and finding the distance from each listing and encoding it as 'distance_to_center'. This led to a dramatic improvement for our models in terms of RMSE. After this success we decided to find more landmarks in Austin area and encode the distance of listings from them in their own factors. Since many landmarks are close together it made sense to create clusters of landmarks and to only record the minimum distance from each listing to one of the locations in that cluster. This, however, only led to a moderate improvement in model performance. The MLP ended up with a RMSE of 159.98. The standard deviation for prices in general were 195.71 which means our model is superior to estimating all airbnb prices with the mean but only somewhat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': mean_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "def classify_feature(feature_name):\n",
        "    if feature_name.startswith('neighbourhood_'):\n",
        "        return 'Neighbourhood'\n",
        "    elif feature_name.startswith('room_type_'):\n",
        "        return 'Room Type'\n",
        "    else:\n",
        "        return feature_name\n",
        "\n",
        "importance_df['Feature_Type'] = importance_df['Feature'].apply(classify_feature)\n",
        "\n",
        "grouped_importance = importance_df.groupby('Feature_Type')['Importance'].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(data=grouped_importance, x='Importance', y='Feature_Type', palette='viridis')\n",
        "plt.title('Total Feature Importance by Group (5-Fold CV)', fontsize=16)\n",
        "plt.xlabel('Total Permutation Importance', fontsize=14)\n",
        "plt.ylabel('Feature Group', fontsize=14)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(grouped_importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at our final variable importance plot from our MLP model we see that the most important features almost all relate to distance. This was expected as location is commonly seen as the most important aspect about real estate. What was suprising however, was how distance to South Congress cluster was more important than distance to downtown cluster. We had expected the distance to downtown cluster to be the most important. Somewhat unsurprisingly, distance to north west cluster was considered of little importance to predicting price. This suggests that proximity to Mount Bonnell is unimportant to most consumers in determining what they are willing to pay for an Airbnb. The most important non-location factor was reviews_per_month. This also makes sense, as having a lot of reviews suggests that consumers on average are more willing to trust the hosts and the listing is more of a hot spot.\n",
        "\n",
        "While we extracted some insights especially about the importance of location on price, our model was only somewhat superior to using a fixed estimator such as the mean in terms of predicting price. This indicates that our features were insufficient to providing an accurate estimate of the price for an Airbnb. Future study should include factors such as photo quality, crime rates, or even seasonal popularity in order to get a more complete picture.\n",
        "\n",
        "Furthermore, it is worth noting that our importance values are all very low. This suggests that the features may be highly correlated with each other, that our data is noisy, or that there is simply a weak relationship between the features and our response variable. If we had more time, we would explore multicolinearity further using tools such as a correlation matrix or a variance inflation factor and remove highly correlated features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "linear_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "\n",
        "lr_mae_list = []\n",
        "lr_rmse_list = []\n",
        "lr_r2_list = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    linear_pipeline.fit(X_train, y_train)\n",
        "    y_pred = linear_pipeline.predict(X_test)\n",
        "\n",
        "    # Revert log transform\n",
        "    y_pred_original = np.expm1(y_pred)\n",
        "    y_test_original = np.expm1(y_test)\n",
        "\n",
        "    # Metrics\n",
        "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
        "    r2 = r2_score(y_test_original, y_pred_original)\n",
        "\n",
        "    lr_mae_list.append(mae)\n",
        "    lr_rmse_list.append(rmse)\n",
        "    lr_r2_list.append(r2)\n",
        "\n",
        "    print(f\"Linear Model - Fold {fold}: MAE={mae:.2f}, RMSE={rmse:.2f}, R²={r2:.3f}\")\n",
        "\n",
        "print(\"\\n=== Linear Regression Model Performance Across Folds ===\")\n",
        "print(f\"Average MAE: {np.mean(lr_mae_list):.2f} ± {np.std(lr_mae_list):.2f}\")\n",
        "print(f\"Average RMSE: {np.mean(lr_rmse_list):.2f} ± {np.std(lr_rmse_list):.2f}\")\n",
        "print(f\"Average R²: {np.mean(lr_r2_list):.3f} ± {np.std(lr_r2_list):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The RMSE of our linear regression model was 171.28. This means that on average, we are about \\$171.27 away from predicting the right price. While it is inferior in terms of our MLP model, we expected it to perform much worse. Initially before our feature engineering, the RMSE of the linear regression model was much worse than for our MLP model. This suggests that in the course of creating new and simpler features from more complicated ones such as longitude and latitude,  we essentially are able to see a more linear relationship between our factors and the response. This is one of the limitations of the linear regression model: the data may not be linear. In addition, our estimates may be unstable since some of the features could be highly correlated. The biggest limitation is that this model does not show us which features are significant. Thus, we need to do further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation**\n",
        "\n",
        "From our feature importance plot, we see that the most important features are Room Type, Distance to the Downtown, South Congress, and Zilker Clusters, and neighborhood (zip code). Thus, we suggest that travelers who are wanting to save money should book listings that are farther from the main hotspots of Austin: Downtown, South Congress, and Zilker Park. Furthermore, highly affluent neighborhoods may also be more expensive than those that are poorer. Finally, travelers who book for listings that are not shared can expect to spend more money."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Contribution Report**\n",
        "\n",
        "Abby contributed to defining the question, the exploratory data analysis, and\n",
        "building the MLP model. Zach contributed to the exploratory data analysis and interpretation.\n",
        "Michael contributed to cleaning the data and building the MLP model. Joyce\n",
        "contributed to cleaning the data and building the linear regression model.\n",
        "We all contributed to the results and conclusion."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
